{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25f93f",
   "metadata": {},
   "source": [
    "# EXP-01( Word Analysis ) 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17557209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'study', '.']\n",
      "Lemmas: ['Natural', 'Language', 'Processing', 'be', 'a', 'fascinating', 'field', 'of', 'study', '.']\n",
      "\n",
      "Dependency Parsing:\n",
      "Natural compound Language PROPN []\n",
      "Language compound Processing PROPN [Natural]\n",
      "Processing nsubj is AUX [Language]\n",
      "is ROOT is AUX [Processing, field, .]\n",
      "a det field NOUN []\n",
      "fascinating amod field NOUN []\n",
      "field attr is AUX [a, fascinating, of]\n",
      "of prep field NOUN [study]\n",
      "study pobj of ADP []\n",
      ". punct is AUX []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Natural Language Processing is a fascinating field of study.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Lemmas:\", lemmas)\n",
    "\n",
    "print(\"\\nDependency Parsing:\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4824ef2",
   "metadata": {},
   "source": [
    "# EXP-02(case study for Word Analysis ) 1b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e34f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Feedback 1: 'The product is amazing! I love the quality.'\n",
      "Tokens: ['The', 'product', 'is', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n",
      "Lemmas: ['the', 'product', 'be', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n",
      "\n",
      "Dependency Parsing:\n",
      "The det product NOUN []\n",
      "product nsubj is AUX [The]\n",
      "is ROOT is AUX [product, amazing, !]\n",
      "amazing acomp is AUX []\n",
      "! punct is AUX []\n",
      "I nsubj love VERB []\n",
      "love ROOT love VERB [I, quality, .]\n",
      "the det quality NOUN []\n",
      "quality dobj love VERB [the]\n",
      ". punct love VERB []\n",
      "\n",
      "Analyzing Feedback 2: 'The delivery was late, very frustrating.'\n",
      "Tokens: ['The', 'delivery', 'was', 'late', ',', 'very', 'frustrating', '.']\n",
      "Lemmas: ['the', 'delivery', 'be', 'late', ',', 'very', 'frustrating', '.']\n",
      "\n",
      "Dependency Parsing:\n",
      "The det delivery NOUN []\n",
      "delivery nsubj was AUX [The]\n",
      "was ROOT was AUX [delivery, frustrating, .]\n",
      "late advmod frustrating ADJ []\n",
      ", punct frustrating ADJ []\n",
      "very advmod frustrating ADJ []\n",
      "frustrating acomp was AUX [late, ,, very]\n",
      ". punct was AUX []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "customer_feedback = [\n",
    " \"The product is amazing! I love the quality.\",\n",
    " \"The delivery was late, very frustrating.\"\n",
    "]\n",
    "\n",
    "def analyze_feedback(feedback):\n",
    "    for idx, text in enumerate(feedback, start=1):\n",
    "        print(f\"\\nAnalyzing Feedback {idx}: '{text}'\")\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Lemmas:\", lemmas)\n",
    "\n",
    "        print(\"\\nDependency Parsing:\")\n",
    "        for token in doc:\n",
    "            print(token.text, token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_feedback(customer_feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a5566",
   "metadata": {},
   "source": [
    "# EXP-03(Word Generation ) 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b75e30ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Lord GOD , whither goest to fear the LORD caused me saw , my father Brown . I see anyone\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "words = nltk.corpus.gutenberg.words()\n",
    "\n",
    "bigrams = list(nltk.bigrams(words))\n",
    "\n",
    "starting_word = \"the\"\n",
    "generated_text = [starting_word]\n",
    "\n",
    "for _ in range(20):\n",
    "    possible_words = [word2 for (word1, word2) in bigrams if word1.lower() == generated_text[-1].lower()]\n",
    "    next_word = random.choice(possible_words)\n",
    "    generated_text.append(next_word)\n",
    "print(' '.join(generated_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fec43b",
   "metadata": {},
   "source": [
    "# EXP-04(case study for word generation) 2b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "170925e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Enter your sentence (type 'exit' to end): hi hoe are you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete Suggestions: ['Hi', '[Recipient],', 'hi', 'hoe', 'are', 'you', 'interested', 'in', 'working', 'on', 'a', 'project', 'that', 'is', 'not', 'a', 'part', 'of', 'the', 'project?', 'I', 'am', 'interested', 'to', 'hear', 'your', 'thoughts', 'on', 'the', 'topic.', 'I', 'have', 'been', 'working', 'with', 'the']\n",
      "Enter your sentence (type 'exit' to end): exit\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class EmailAutocompleteSystem:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"gpt2\"\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
    "    \n",
    "    def generate_suggestions(self, user_input, context):\n",
    "        input_text = f\"{context} {user_input}\"\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            suggestions = generated_text.split()[len(user_input.split()):]\n",
    "        return suggestions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    autocomplete_system = EmailAutocompleteSystem()\n",
    "    \n",
    "    email_context = \"Subject: Discussing Project Proposal\\nHi [Recipient],\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your sentence (type 'exit' to end): \")\n",
    "\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        suggestions = autocomplete_system.generate_suggestions(user_input, email_context)\n",
    "\n",
    "        if suggestions:\n",
    "            print(\"Autocomplete Suggestions:\", suggestions)\n",
    "        else:\n",
    "            print(\"No suggestions available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d1dd6",
   "metadata": {},
   "source": [
    "# EXP-05(TEXT CLASSIFICATION) 3a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c3a0a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Accuracy: 0.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       0.0\n",
      "    positive       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\New folder\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\New folder\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\New folder\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\New folder\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\New folder\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\New folder\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "data = {\n",
    " 'text': [\n",
    " 'This is a positive sentence',\n",
    " 'I am happy today',\n",
    " 'Negative review, very bad service',\n",
    " 'I do not like this product'\n",
    " ],\n",
    " 'label': ['positive', 'positive', 'negative', 'negative']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2,\n",
    "random_state=42)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d71191",
   "metadata": {},
   "source": [
    "# EXP-06(case study for TEXT CLASSIFICATION) 3b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a587bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Accuracy: 0.9504823151125402\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       389\n",
      "           1       0.96      0.91      0.94       396\n",
      "           2       0.98      0.94      0.96       394\n",
      "           3       0.98      0.98      0.98       376\n",
      "\n",
      "    accuracy                           0.95      1555\n",
      "   macro avg       0.95      0.95      0.95      1555\n",
      "weighted avg       0.95      0.95      0.95      1555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "categories = ['sci.med', 'sci.space', 'comp.graphics', 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(),LinearSVC())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184c300",
   "metadata": {},
   "source": [
    "# EXP-07(semantic analysis) 4a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5514faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Words similar to 'natural': [('Splittorff_lacked', 0.636509358882904), ('Natural', 0.58078932762146), ('Mike_Taugher_covers', 0.577259361743927), ('manmade', 0.5276211500167847), ('shell_salted_pistachios', 0.5084421634674072), ('unnatural', 0.5030758380889893), ('naturally', 0.49992606043815613), ('Intraparty_squabbles', 0.4988228678703308), ('Burt_Bees_®', 0.49539363384246826), ('causes_Buxeda', 0.4935200810432434)]\n",
      "Words similar to 'language': [('langauge', 0.7476695775985718), ('Language', 0.6695356369018555), ('languages', 0.6341332197189331), ('English', 0.6120712757110596), ('CMPB_Spanish', 0.6083104610443115), ('nonnative_speakers', 0.6063109636306763), ('idiomatic_expressions', 0.5889801979064941), ('verb_tenses', 0.58415687084198), ('Kumeyaay_Diegueno', 0.5798824429512024), ('dialect', 0.5724600553512573)]\n",
      "Words similar to 'processing': [('Processing', 0.7285515666007996), ('processed', 0.6519132852554321), ('processor', 0.636760413646698), ('warden_Dominick_DeRose', 0.6166526675224304), ('processors', 0.5953895449638367), ('Discoverer_Enterprise_resumed', 0.5376213192939758), ('LSI_Tarari', 0.520267903804779), ('processer', 0.5166687369346619), ('remittance_processing', 0.5144169926643372), ('Farmland_Foods_pork', 0.5071728825569153)]\n",
      "Words similar to 'is': [('was', 0.6549733281135559), (\"isn'ta\", 0.6439523100852966), ('seems', 0.634029746055603), ('Is', 0.6085968613624573), ('becomes', 0.5841935276985168), ('appears', 0.5822900533676147), ('remains', 0.5796942114830017), ('іѕ', 0.5695518255233765), ('makes', 0.5567088723182678), ('isn_`_t', 0.5513144135475159)]\n",
      "'a' is not in the pre-trained Word2Vec model.\n",
      "Words similar to 'challenging': [('difficult', 0.6388775110244751), ('challenge', 0.5953003764152527), ('daunting', 0.569800615310669), ('tough', 0.5689979791641235), ('challenges', 0.5471934676170349), ('challenged', 0.5449535846710205), ('Challenging', 0.5242965817451477), ('tricky', 0.5236554741859436), ('toughest', 0.5169045329093933), ('diffi_cult', 0.5010539889335632)]\n",
      "Words similar to 'but': [('although', 0.8104525804519653), ('though', 0.7285684943199158), ('because', 0.7225914597511292), ('so', 0.6865807771682739), ('But', 0.6826984882354736), ('Although', 0.6188263297080994), ('Though', 0.6153667569160461), ('Unfortunately', 0.6031029224395752), ('Of_course', 0.593142032623291), ('anyway', 0.5869061350822449)]\n",
      "Words similar to 'fascinating': [('interesting', 0.7623067498207092), ('intriguing', 0.7245113253593445), ('enlightening', 0.6644250154495239), ('captivating', 0.6459898352622986), ('facinating', 0.6416683793067932), ('riveting', 0.6324825286865234), ('instructive', 0.6210989356040955), ('endlessly_fascinating', 0.6188612580299377), ('revelatory', 0.6170244216918945), ('engrossing', 0.6126049160957336)]\n",
      "Words similar to 'field': [('fields', 0.5582526326179504), ('fi_eld', 0.5188260078430176), ('Keith_Toogood', 0.49749255180358887), ('Mackenzie_Hoambrecker', 0.49514278769493103), ('Josh_Arauco_kicked', 0.48817265033721924), ('Nick_Cattoi', 0.4863145053386688), ('Armando_Cuko', 0.4853871166706085), ('Jon_Striefsky', 0.48322004079818726), ('kicker_Nico_Grasu', 0.47572532296180725), ('Chris_Manfredini_kicked', 0.47327715158462524)]\n",
      "'.' is not in the pre-trained Word2Vec model.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk\n",
    "\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "sentences = [\n",
    "\"Natural language processing is a challenging but fascinating field.\"]\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "for tokenized_sentence in tokenized_sentences:\n",
    "    for word in tokenized_sentence:\n",
    "        if word in word_vectors:\n",
    "            similar_words = word_vectors.most_similar(word)\n",
    "            print(f\"Words similar to '{word}': {similar_words}\")\n",
    "        else:\n",
    "            print(f\"'{word}' is not in the pre-trained Word2Vec model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4d9ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook --generate-config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d07b4",
   "metadata": {},
   "source": [
    "# EXP-08(case study for semantic analysis) 4b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cce7934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Query: I received a damaged product. Can I get a refund?\n",
      "Semantic Analysis (Synonyms): ['amaze', 'refund', 'merchandise', 'perplex', 'stupefy', 'damage', 'capture', 'welcome', 'product', 'receive', 'bewilder', 'buzz_off', 'acquire', 'Cartesian_product', 'suffer', 'scram', 'beat', 'experience', 'get_down', 'begin', 'cause', 'puzzle', 'pick_up', 'fuck_off', 'make', 'pay_off', 'invite', 'encounter', 'take', 'incur', 'grow', 'nonplus', 'bring_forth', 'have', 'sustain', 'generate', 'mystify', 'damaged', 'discredited', 'vex', 'stimulate', 'start_out', 'take_in', 'become', 'contract', 'mother', 'baffle', 'aim', 'bugger_off', 'commence', 'beget', 'pay_back', 'pose', 'start', 'production', 'gravel', 'give_back', 'get', 'let', 'received', 'go', 'induce', 'develop', 'father', 'arrest', 'bring', 'fix', 'dumbfound', 'repay', 'flummox', 'meet', 'draw', 'fetch', \"get_under_one's_skin\", 'catch', 'obtain', 'return', 'set_about', 'produce', 'ware', 'set_out', 'engender', 'drive', 'find', 'sire', 'stick', 'come', 'repayment', 'arrive', 'mathematical_product', 'intersection', 'standard', 'convey']\n",
      "\n",
      "\n",
      "Customer Query: I'm having trouble accessing my account.\n",
      "Semantic Analysis (Synonyms): ['disoblige', 'pain', 'unhinge', 'invoice', 'business_relationship', 'disturb', 'problem', 'describe', 'trouble_oneself', 'inconvenience', 'history', 'accounting', 'write_up', 'hassle', 'worry', 'explanation', 'bill', 'news_report', 'disorder', 'ail', 'score', 'chronicle', 'calculate', 'distract', 'trouble', 'bother', 'perturb', 'upset', 'cark', 'disquiet', 'put_out', 'story', 'access', 'account', 'inconvenience_oneself', 'report', 'answer_for', 'fuss', 'difficulty', 'incommode', 'get_at', 'account_statement', 'discommode']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def semantic_analysis(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    synonyms = set()\n",
    "    \n",
    "    for token in lemmatized_tokens:\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "customer_queries = [\n",
    "    \"I received a damaged product. Can I get a refund?\",\n",
    "    \"I'm having trouble accessing my account.\",\n",
    "\n",
    "]\n",
    "for query in customer_queries:\n",
    "    print(\"Customer Query:\", query)\n",
    "    synonyms = semantic_analysis(query)\n",
    "    print(\"Semantic Analysis (Synonyms):\", synonyms)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae3d18",
   "metadata": {},
   "source": [
    "# EXP-09 (sentiment analysis) 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0c0ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.85      0.84       199\n",
      "         pos       0.85      0.82      0.84       201\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import movie_reviews \n",
    "\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    " for category in movie_reviews.categories()\n",
    " for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "df = pd.DataFrame(documents, columns=['text', 'sentiment'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2,random_state=42)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.apply(' '.join))\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test.apply(' '.join))\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf8a6d",
   "metadata": {},
   "source": [
    "# EXP-10(case study for sentiment analysis) 5b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d447a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This product is amazing! I love it.\n",
      "Review: The product was good, but the packaging was damaged.\n",
      "Review: Very disappointing experience. Would not recommend.\n",
      "Review: Neutral feedback on the product.\n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "reviews = [\n",
    "\"This product is amazing! I love it.\",\n",
    "\"The product was good, but the packaging was damaged.\",\n",
    "\"Very disappointing experience. Would not recommend.\",\n",
    "\"Neutral feedback on the product.\",\n",
    "]\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for review in reviews:\n",
    "    print(\"Review:\", review)\n",
    "    scores = sid.polarity_scores(review)\n",
    "print(\"Sentiment:\", end=' ')\n",
    "if scores['compound'] > 0.05:\n",
    "    print(\"Positive\")\n",
    "elif scores['compound'] < -0.05:\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Neutral\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321465d",
   "metadata": {},
   "source": [
    "# EXP-11(pos tagging) 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9da37f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "POS tags: [('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('tagging', 'VBG'), ('helps', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('function', 'NN'), ('of', 'IN'), ('each', 'DT'), ('word', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"Parts of speech tagging helps to understand the function of each word in a sentence.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"POS tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17672be5",
   "metadata": {},
   "source": [
    "# EXP-12(case study for pos tagging) 6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cbb77a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article Text:\n",
      " \n",
      "    The victory boosts United's chances in the Premier League title race.\n",
      "    \n",
      "\n",
      "Parts of Speech Tagging:\n",
      "The: DT\n",
      "victory: NN\n",
      "boosts: VBZ\n",
      "United: NNP\n",
      "'s: POS\n",
      "chances: NNS\n",
      "in: IN\n",
      "the: DT\n",
      "Premier: NNP\n",
      "League: NNP\n",
      "title: NN\n",
      "race: NN\n",
      ".: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tagged_tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tagged_tokens.extend(nltk.pos_tag(tokens))\n",
    "    return tagged_tokens\n",
    "\n",
    "def main():\n",
    "    article_text = \"\"\"\n",
    "    The victory boosts United's chances in the Premier League title race.\n",
    "    \"\"\"\n",
    "    tagged_tokens = pos_tagging(article_text)\n",
    "    print(\"Original Article Text:\\n\", article_text)\n",
    "    print(\"\\nParts of Speech Tagging:\")\n",
    "    for token, pos_tag in tagged_tokens:\n",
    "        print(f\"{token}: {pos_tag}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b10c78",
   "metadata": {},
   "source": [
    "# EXP-13(Chunking) 7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f905c7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\new folder\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "(NP The/DT quick/JJ brown/NN)\n",
      "(NP fox/NN)\n",
      "(NP the/DT lazy/JJ dog/NN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk import RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "chunk_grammar = r\"\"\"\n",
    " NP: {<DT>?<JJ>*<NN>}\n",
    "\"\"\"\n",
    "\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "\n",
    "chunks = chunk_parser.parse(tagged)\n",
    "\n",
    "for subtree in chunks.subtrees():\n",
    "    if subtree.label() == 'NP':\n",
    "        print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908265c",
   "metadata": {},
   "source": [
    "# EXP-14(case study for chunking) 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d15b643f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown fox jumps over the lazy dog.\n",
      "Noun Phrases:\n",
      "- The quick brown\n",
      "- fox\n",
      "- the lazy dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.data.path.append(\"/usr/local/share/nltk_data\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "chunk_grammar = r\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} # Chunk sequences of DT, JJ, NN\n",
    "\"\"\"\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "\n",
    "chunked_text = chunk_parser.parse(pos_tags)\n",
    "\n",
    "noun_phrases = []\n",
    "for subtree in chunked_text.subtrees(filter=lambda t: t.label() =='NP'):\n",
    "    noun_phrases.append(' '.join(word for word, tag in subtree.leaves()))\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Noun Phrases:\")\n",
    "for phrase in noun_phrases:\n",
    "    print(\"-\", phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
